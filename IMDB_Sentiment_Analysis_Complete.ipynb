{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMDb Movie Review Sentiment Analysis - Complete Pipeline\n",
        "\n",
        "## Overview\n",
        "This notebook contains the complete end-to-end pipeline for IMDb movie review sentiment analysis:\n",
        "1. Data Loading and Exploration\n",
        "2. Data Preprocessing and Cleaning\n",
        "3. Feature Engineering (TF-IDF, Word2Vec, Textual Features)\n",
        "4. Model Development (Multiple Algorithms)\n",
        "5. Model Evaluation and Visualization\n",
        "6. Predictions on New Reviews\n",
        "\n",
        "## Dataset\n",
        "The dataset contains movie reviews with sentiment labels (positive/negative).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Text processing\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Advanced ML\n",
        "from xgboost import XGBClassifier\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Download NLTK data (run once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('models/visualizations', exist_ok=True)\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "print(\"✓ NLTK data downloaded/verified!\")\n",
        "print(\"✓ Directories created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Data Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data/imdb_data.csv')\n",
        "\n",
        "print(f\"✓ Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(f\"Column names: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display column information\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Count': missing_values.values,\n",
        "    'Missing Percentage': missing_percent.values\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"Missing Values Found:\")\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"✓ No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\nRemoving duplicates...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"✓ Dataset shape after removing duplicates: {df.shape}\")\n",
        "else:\n",
        "    print(\"✓ No duplicates found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment distribution\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "sentiment_percentages = df['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(sentiment_counts)\n",
        "print(\"\\nSentiment Percentages:\")\n",
        "print(sentiment_percentages)\n",
        "\n",
        "# Visualize sentiment distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Bar chart\n",
        "sentiment_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "sentiment_percentages.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
        "axes[1].set_title('Sentiment Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate review length statistics\n",
        "df['review_length'] = df['review'].str.len()\n",
        "df['word_count'] = df['review'].str.split().str.len()\n",
        "\n",
        "print(\"Review Length Statistics:\")\n",
        "print(df[['review_length', 'word_count']].describe())\n",
        "\n",
        "# Visualize review length distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Character count distribution\n",
        "df.boxplot(column='review_length', by='sentiment', ax=axes[0])\n",
        "axes[0].set_title('Review Length (Characters) by Sentiment', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Sentiment', fontsize=10)\n",
        "axes[0].set_ylabel('Character Count', fontsize=10)\n",
        "\n",
        "# Word count distribution\n",
        "df.boxplot(column='word_count', by='sentiment', ax=axes[1])\n",
        "axes[1].set_title('Word Count by Sentiment', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Sentiment', fontsize=10)\n",
        "axes[1].set_ylabel('Word Count', fontsize=10)\n",
        "\n",
        "plt.suptitle('Review Length Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/review_length_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word clouds for positive and negative reviews\n",
        "def create_wordcloud(text, title, ax):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Combine all positive reviews\n",
        "positive_text = ' '.join(df[df['sentiment'] == 'positive']['review'].astype(str))\n",
        "\n",
        "# Combine all negative reviews\n",
        "negative_text = ' '.join(df[df['sentiment'] == 'negative']['review'].astype(str))\n",
        "\n",
        "# Create word clouds\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "create_wordcloud(positive_text, 'Positive Reviews Word Cloud', axes[0])\n",
        "create_wordcloud(negative_text, 'Negative Reviews Word Cloud', axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/wordclouds.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string\n",
        "    text = str(text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove special characters and digits (keep only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize text into words\"\"\"\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Remove stop words from tokens\"\"\"\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "def lemmatize_text(tokens):\n",
        "    \"\"\"Lemmatize tokens (convert to base form)\"\"\"\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def preprocess_text(text, use_lemmatization=True, remove_stop=True):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    # Clean text\n",
        "    text = clean_text(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenize_text(text)\n",
        "    \n",
        "    # Remove stop words\n",
        "    if remove_stop:\n",
        "        tokens = remove_stopwords(tokens)\n",
        "    \n",
        "    # Lemmatize\n",
        "    if use_lemmatization:\n",
        "        tokens = lemmatize_text(tokens)\n",
        "    \n",
        "    # Join tokens back to string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"✓ Text preprocessing functions created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply preprocessing to reviews\n",
        "print(\"Starting text preprocessing...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "# Create a copy of the dataframe\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Apply preprocessing (using lemmatization)\n",
        "df_processed['cleaned_review'] = df_processed['review'].apply(\n",
        "    lambda x: preprocess_text(x, use_lemmatization=True, remove_stop=True)\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Text preprocessing completed!\")\n",
        "print(f\"\\nSample original review:\")\n",
        "print(df_processed['review'].iloc[0][:200])\n",
        "print(f\"\\nSample cleaned review:\")\n",
        "print(df_processed['cleaned_review'].iloc[0][:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract various textual features\n",
        "def extract_text_features(df):\n",
        "    \"\"\"Extract textual features from reviews\"\"\"\n",
        "    features = df.copy()\n",
        "    \n",
        "    # Basic length features\n",
        "    features['char_count'] = features['cleaned_review'].str.len()\n",
        "    features['word_count'] = features['cleaned_review'].str.split().str.len()\n",
        "    features['sentence_count'] = features['cleaned_review'].str.split('.').str.len()\n",
        "    \n",
        "    # Average word length\n",
        "    features['avg_word_length'] = features['char_count'] / (features['word_count'] + 1)\n",
        "    \n",
        "    # Count of uppercase letters (if any remain after preprocessing)\n",
        "    features['uppercase_count'] = features['cleaned_review'].str.findall(r'[A-Z]').str.len()\n",
        "    \n",
        "    # Count of digits (if any remain)\n",
        "    features['digit_count'] = features['cleaned_review'].str.findall(r'\\d').str.len()\n",
        "    \n",
        "    # Count of special characters\n",
        "    features['special_char_count'] = features['cleaned_review'].str.findall(r'[^a-zA-Z0-9\\s]').str.len()\n",
        "    \n",
        "    # Count of exclamation marks and question marks (sentiment indicators)\n",
        "    features['exclamation_count'] = features['cleaned_review'].str.count('!')\n",
        "    features['question_count'] = features['cleaned_review'].str.count('?')\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Extract features\n",
        "df_features = extract_text_features(df_processed)\n",
        "\n",
        "print(\"✓ Textual features extracted!\")\n",
        "print(\"\\nFeature Statistics:\")\n",
        "print(df_features[['char_count', 'word_count', 'avg_word_length']].describe())\n",
        "print(\"\\nSample features:\")\n",
        "df_features[['cleaned_review', 'char_count', 'word_count', 'avg_word_length']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "# Using common parameters for text classification\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Top 5000 features\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    min_df=2,  # Minimum document frequency\n",
        "    max_df=0.95,  # Maximum document frequency (ignore very common words)\n",
        "    sublinear_tf=True  # Apply sublinear tf scaling\n",
        ")\n",
        "\n",
        "# Fit and transform the cleaned reviews\n",
        "print(\"Fitting TF-IDF vectorizer...\")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_features['cleaned_review'])\n",
        "\n",
        "print(f\"✓ TF-IDF matrix shape: {X_tfidf.shape}\")\n",
        "print(f\"Number of features: {X_tfidf.shape[1]}\")\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample feature names (first 20):\")\n",
        "print(feature_names[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare tokenized sentences for Word2Vec\n",
        "tokenized_reviews = [review.split() for review in df_features['cleaned_review']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"Training Word2Vec model...\")\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_reviews,\n",
        "    vector_size=100,  # Dimension of word vectors\n",
        "    window=5,  # Context window size\n",
        "    min_count=2,  # Minimum word frequency\n",
        "    workers=4,  # Number of threads\n",
        "    sg=0  # 0 for CBOW, 1 for Skip-gram\n",
        ")\n",
        "\n",
        "print(f\"✓ Word2Vec model trained!\")\n",
        "print(f\"Vocabulary size: {len(word2vec_model.wv.key_to_index)}\")\n",
        "\n",
        "# Create document vectors by averaging word vectors\n",
        "def get_document_vector(words, model):\n",
        "    \"\"\"Get document vector by averaging word vectors\"\"\"\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Create document vectors\n",
        "print(\"Creating document vectors...\")\n",
        "X_word2vec = np.array([get_document_vector(review, word2vec_model) for review in tokenized_reviews])\n",
        "\n",
        "print(f\"✓ Word2Vec matrix shape: {X_word2vec.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract textual features as numpy array\n",
        "textual_features = df_features[['char_count', 'word_count', 'avg_word_length', \n",
        "                                 'exclamation_count', 'question_count']].values\n",
        "\n",
        "# Normalize textual features\n",
        "scaler = StandardScaler()\n",
        "textual_features_scaled = scaler.fit_transform(textual_features)\n",
        "\n",
        "print(f\"Textual features shape: {textual_features_scaled.shape}\")\n",
        "\n",
        "# Combine TF-IDF with textual features\n",
        "X_combined_tfidf = hstack([X_tfidf, textual_features_scaled])\n",
        "\n",
        "print(f\"✓ Combined TF-IDF + Textual features shape: {X_combined_tfidf.shape}\")\n",
        "\n",
        "# Combine Word2Vec with textual features\n",
        "X_combined_word2vec = np.hstack([X_word2vec, textual_features_scaled])\n",
        "\n",
        "print(f\"✓ Combined Word2Vec + Textual features shape: {X_combined_word2vec.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df_features['sentiment'])\n",
        "\n",
        "print(f\"✓ Target variable shape: {y.shape}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "print(f\"\\nLabel mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {i}: {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for TF-IDF features\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
        "    X_combined_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Get indices for train and test sets\n",
        "train_indices, test_indices = train_test_split(\n",
        "    np.arange(len(y)), test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Split data for Word2Vec features\n",
        "X_train_word2vec, X_test_word2vec, _, _ = train_test_split(\n",
        "    X_combined_word2vec, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"✓ Data split completed!\")\n",
        "print(f\"Training set size (TF-IDF): {X_train_tfidf.shape}\")\n",
        "print(f\"Test set size (TF-IDF): {X_test_tfidf.shape}\")\n",
        "print(f\"Training labels: {y_train.shape}\")\n",
        "print(f\"Test labels: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Model Development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Train a model and evaluate its performance\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = None\n",
        "    \n",
        "    # Get prediction probabilities if available\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    roc_auc = None\n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Training Time: {training_time:.2f} seconds\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'training_time': training_time,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "print(\"✓ Model training function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "results_tfidf = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 1: Logistic Regression\")\n",
        "print(\"=\"*60)\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "results_tfidf['Logistic Regression'] = train_and_evaluate_model(\n",
        "    lr_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Logistic Regression\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Naive Bayes\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 2: Naive Bayes\")\n",
        "print(\"=\"*60)\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "results_tfidf['Naive Bayes'] = train_and_evaluate_model(\n",
        "    nb_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Naive Bayes\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 3: Support Vector Machine\")\n",
        "print(\"=\"*60)\n",
        "# Note: SVM can be slow on large datasets, using linear kernel for speed\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "results_tfidf['SVM'] = train_and_evaluate_model(\n",
        "    svm_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"SVM\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Random Forest\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 4: Random Forest\")\n",
        "print(\"=\"*60)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
        "results_tfidf['Random Forest'] = train_and_evaluate_model(\n",
        "    rf_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Random Forest\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. XGBoost\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 5: XGBoost\")\n",
        "print(\"=\"*60)\n",
        "# Convert sparse matrix to dense for XGBoost\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray() if hasattr(X_train_tfidf, 'toarray') else X_train_tfidf\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray() if hasattr(X_test_tfidf, 'toarray') else X_test_tfidf\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "results_tfidf['XGBoost'] = train_and_evaluate_model(\n",
        "    xgb_model, X_train_tfidf_dense, X_test_tfidf_dense, y_train, y_test, \"XGBoost\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision': [],\n",
        "    'Recall': [],\n",
        "    'F1-Score': [],\n",
        "    'ROC-AUC': [],\n",
        "    'Training Time (s)': []\n",
        "}\n",
        "\n",
        "for model_name, result in results_tfidf.items():\n",
        "    comparison_data['Model'].append(model_name)\n",
        "    comparison_data['Accuracy'].append(result['accuracy'])\n",
        "    comparison_data['Precision'].append(result['precision'])\n",
        "    comparison_data['Recall'].append(result['recall'])\n",
        "    comparison_data['F1-Score'].append(result['f1_score'])\n",
        "    comparison_data['ROC-AUC'].append(result['roc_auc'] if result['roc_auc'] is not None else np.nan)\n",
        "    comparison_data['Training Time (s)'].append(result['training_time'])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON (TF-IDF Features)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    comparison_df_sorted = comparison_df.sort_values(metric, ascending=True)\n",
        "    ax.barh(comparison_df_sorted['Model'], comparison_df_sorted[metric], color='steelblue')\n",
        "    ax.set_xlabel(metric, fontsize=12)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "    for i, v in enumerate(comparison_df_sorted[metric]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/model_comparison_tfidf.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "print(f\"\\n✓ Best performing model: {best_model_name}\")\n",
        "\n",
        "# Hyperparameter tuning for best model\n",
        "print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear']\n",
        "    }\n",
        "    base_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "    \n",
        "elif best_model_name == 'Naive Bayes':\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 0.5, 1.0, 2.0]\n",
        "    }\n",
        "    base_model = MultinomialNB()\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "    \n",
        "elif best_model_name == 'SVM':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    }\n",
        "    base_model = SVC(probability=True, random_state=42)\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "    \n",
        "elif best_model_name == 'Random Forest':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "    \n",
        "elif best_model_name == 'XGBoost':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "    base_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "    X_train_tuned = X_train_tfidf_dense\n",
        "    X_test_tuned = X_test_tfidf_dense\n",
        "\n",
        "# Perform grid search\n",
        "print(\"Running GridSearchCV (this may take a while)...\")\n",
        "grid_search = GridSearchCV(\n",
        "    base_model, \n",
        "    param_grid, \n",
        "    cv=3, \n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_tuned, y_train)\n",
        "\n",
        "print(f\"\\n✓ Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "best_tuned_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_tuned_model.predict(X_test_tuned)\n",
        "\n",
        "print(f\"\\nTuned {best_model_name} Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"  F1-Score: {f1_score(y_test, y_pred_tuned, average='weighted'):.4f}\")\n",
        "\n",
        "# Update results\n",
        "results_tfidf[f'{best_model_name} (Tuned)'] = {\n",
        "    'model': best_tuned_model,\n",
        "    'model_name': f'{best_model_name} (Tuned)',\n",
        "    'accuracy': accuracy_score(y_test, y_pred_tuned),\n",
        "    'precision': precision_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'recall': recall_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'f1_score': f1_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'roc_auc': roc_auc_score(y_test, best_tuned_model.predict_proba(X_test_tuned)[:, 1]) if hasattr(best_tuned_model, 'predict_proba') else None,\n",
        "    'training_time': 0,\n",
        "    'y_pred': y_pred_tuned,\n",
        "    'y_pred_proba': best_tuned_model.predict_proba(X_test_tuned)[:, 1] if hasattr(best_tuned_model, 'predict_proba') else None\n",
        "}\n",
        "\n",
        "# Update best model\n",
        "best_model = best_tuned_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all models\n",
        "for model_name, result in results_tfidf.items():\n",
        "    # Clean model name for filename\n",
        "    filename = model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('_tuned', '_tuned')\n",
        "    filepath = f'models/{filename}.pkl'\n",
        "    \n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(result['model'], f)\n",
        "    \n",
        "    print(f\"Saved: {filepath}\")\n",
        "\n",
        "# Save best model separately\n",
        "with open('models/best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "# Save vectorizers and encoders\n",
        "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "word2vec_model.save('models/word2vec_model.model')\n",
        "\n",
        "with open('models/feature_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(f\"\\n✓ Best model saved: models/best_model.pkl ({best_model_name})\")\n",
        "print(\"✓ All models and vectorizers saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions with best model\n",
        "y_pred = best_model.predict(X_test_tuned)\n",
        "y_pred_proba = best_model.predict_proba(X_test_tuned)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "print(\"✓ Predictions made successfully!\")\n",
        "print(f\"Prediction shape: {y_pred.shape}\")\n",
        "if y_pred_proba is not None:\n",
        "    print(f\"Prediction probabilities shape: {y_pred_proba.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate all metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
        "\n",
        "# Print metrics\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "if roc_auc is not None:\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add percentage annotations\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "for i in range(len(label_encoder.classes_)):\n",
        "    for j in range(len(label_encoder.classes_)):\n",
        "        ax.text(j+0.5, i+0.7, f'({cm_percent[i,j]:.1f}%)',\n",
        "                ha='center', va='center', fontsize=9, color='red', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if y_pred_proba is not None:\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "else:\n",
        "    print(\"ROC curve not available (model doesn't support probability predictions)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if y_pred_proba is not None:\n",
        "    # Calculate Precision-Recall curve\n",
        "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall_vals, precision_vals)\n",
        "    \n",
        "    # Plot Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall_vals, precision_vals, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "    plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
        "    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower left\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Precision-Recall AUC Score: {pr_auc:.4f}\")\n",
        "else:\n",
        "    print(\"Precision-Recall curve not available (model doesn't support probability predictions)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance if available (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importances = best_model.feature_importances_\n",
        "    \n",
        "    # Get top 20 most important features (excluding textual features)\n",
        "    n_tfidf_features = X_tfidf.shape[1]\n",
        "    tfidf_importances = feature_importances[:n_tfidf_features]\n",
        "    top_indices = np.argsort(tfidf_importances)[-20:][::-1]\n",
        "    top_features = [(feature_names[i], tfidf_importances[i]) for i in top_indices]\n",
        "    \n",
        "    # Visualize top features\n",
        "    features_df = pd.DataFrame(top_features, columns=['Feature', 'Importance'])\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(data=features_df, y='Feature', x='Importance', palette='viridis')\n",
        "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "    plt.title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(features_df.to_string(index=False))\n",
        "    \n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # For linear models, use coefficients\n",
        "    coef = best_model.coef_[0]\n",
        "    \n",
        "    # Get top 20 features (positive and negative)\n",
        "    n_tfidf_features = X_tfidf.shape[1]\n",
        "    tfidf_coef = coef[:n_tfidf_features]\n",
        "    \n",
        "    top_positive_indices = np.argsort(tfidf_coef)[-10:][::-1]\n",
        "    top_negative_indices = np.argsort(tfidf_coef)[:10]\n",
        "    \n",
        "    print(\"\\nTop 10 Features for Positive Sentiment:\")\n",
        "    for idx in top_positive_indices:\n",
        "        print(f\"  {feature_names[idx]}: {tfidf_coef[idx]:.4f}\")\n",
        "    \n",
        "    print(\"\\nTop 10 Features for Negative Sentiment:\")\n",
        "    for idx in top_negative_indices:\n",
        "        print(f\"  {feature_names[idx]}: {tfidf_coef[idx]:.4f}\")\n",
        "else:\n",
        "    print(\"Feature importance not available for this model type.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze misclassified examples\n",
        "df_results = pd.DataFrame({\n",
        "    'review': df_processed['review'].iloc[test_indices].values,\n",
        "    'sentiment': df_processed['sentiment'].iloc[test_indices].values,\n",
        "    'predicted': label_encoder.inverse_transform(y_pred),\n",
        "    'correct': (y_test == y_pred)\n",
        "})\n",
        "\n",
        "misclassified = df_results[df_results['correct'] == False]\n",
        "\n",
        "print(f\"Total misclassified reviews: {len(misclassified)}\")\n",
        "print(f\"Misclassification rate: {len(misclassified) / len(df_results) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nMisclassification breakdown:\")\n",
        "print(misclassified.groupby(['sentiment', 'predicted']).size().unstack(fill_value=0))\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE MISCLASSIFIED REVIEWS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for idx, row in misclassified.head(10).iterrows():\n",
        "    print(f\"\\nActual: {row['sentiment']} | Predicted: {row['predicted']}\")\n",
        "    print(f\"Review: {row['review'][:200]}...\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Predictions on New Reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(review_text):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given review text\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    review_text : str\n",
        "        The movie review text to analyze\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing prediction, probability, and confidence\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(review_text)\n",
        "    \n",
        "    # Extract textual features\n",
        "    char_count = len(cleaned_text)\n",
        "    word_count = len(cleaned_text.split())\n",
        "    avg_word_length = char_count / (word_count + 1) if word_count > 0 else 0\n",
        "    exclamation_count = cleaned_text.count('!')\n",
        "    question_count = cleaned_text.count('?')\n",
        "    \n",
        "    # Transform text using TF-IDF\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "    \n",
        "    # Scale textual features\n",
        "    textual_features = np.array([[char_count, word_count, avg_word_length, \n",
        "                                  exclamation_count, question_count]])\n",
        "    textual_features_scaled = scaler.transform(textual_features)\n",
        "    \n",
        "    # Combine features\n",
        "    features = hstack([text_tfidf, textual_features_scaled])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(features)[0]\n",
        "    sentiment = label_encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    # Get prediction probability if available\n",
        "    if hasattr(best_model, 'predict_proba'):\n",
        "        probabilities = best_model.predict_proba(features)[0]\n",
        "        prob_dict = {label_encoder.classes_[i]: probabilities[i] \n",
        "                    for i in range(len(label_encoder.classes_))}\n",
        "        confidence = max(probabilities)\n",
        "    else:\n",
        "        prob_dict = None\n",
        "        confidence = None\n",
        "    \n",
        "    return {\n",
        "        'sentiment': sentiment,\n",
        "        'prediction': prediction,\n",
        "        'probabilities': prob_dict,\n",
        "        'confidence': confidence,\n",
        "        'original_text': review_text,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }\n",
        "\n",
        "print(\"✓ Prediction function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample reviews for testing\n",
        "sample_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting was superb and the storyline kept me engaged throughout. Highly recommended!\",\n",
        "    \"I was really disappointed with this film. The plot was confusing and the characters were poorly developed. Not worth watching.\",\n",
        "    \"The movie was okay. Nothing special, but not terrible either. It's a decent watch if you have nothing else to do.\",\n",
        "    \"Amazing cinematography and brilliant performances by all actors. This is one of the best movies I've seen this year!\",\n",
        "    \"Terrible movie. Boring plot, bad acting, and a complete waste of time. I would not recommend this to anyone.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICTING SENTIMENT FOR SAMPLE REVIEWS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    result = predict_sentiment(review)\n",
        "    \n",
        "    print(f\"\\nReview {i}:\")\n",
        "    print(f\"Text: {review[:100]}...\")\n",
        "    print(f\"Predicted Sentiment: {result['sentiment'].upper()}\")\n",
        "    if result['probabilities']:\n",
        "        print(f\"Probabilities: {result['probabilities']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Accomplishments:\n",
        "1. ✅ **Data Loading & Exploration**: Loaded dataset, analyzed sentiment distribution, and explored review characteristics\n",
        "2. ✅ **Text Preprocessing**: Cleaned text, removed HTML tags, tokenized, removed stopwords, and lemmatized\n",
        "3. ✅ **Feature Engineering**: Created TF-IDF vectors, Word2Vec embeddings, and extracted textual features\n",
        "4. ✅ **Model Development**: Trained 5 different models (Logistic Regression, Naive Bayes, SVM, Random Forest, XGBoost)\n",
        "5. ✅ **Model Evaluation**: Comprehensive evaluation with confusion matrix, ROC curves, and feature importance\n",
        "6. ✅ **Predictions**: Created prediction function for new reviews\n",
        "\n",
        "### Model Performance:\n",
        "- Best model selected based on F1-Score\n",
        "- Hyperparameter tuning performed\n",
        "- All models saved for future use\n",
        "\n",
        "### Next Steps:\n",
        "- Deploy model for production use\n",
        "- Create API for real-time predictions\n",
        "- Monitor model performance over time\n",
        "- Retrain periodically with new data\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
