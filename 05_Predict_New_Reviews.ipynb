{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMDb Movie Review Sentiment Analysis - Part 5: Predict New Reviews\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates how to use the trained model to predict sentiment for new movie reviews.\n",
        "\n",
        "## Steps:\n",
        "1. Load trained model and vectorizers\n",
        "2. Preprocess new review text\n",
        "3. Make predictions\n",
        "4. Get prediction probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Load Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data if needed\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Initialize text preprocessing components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = clean_text(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Libraries and preprocessing functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Trained Model and Vectorizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TF-IDF vectorizer\n",
        "with open('models/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Load feature scaler\n",
        "with open('models/feature_scaler.pkl', 'rb') as f:\n",
        "    feature_scaler = pickle.load(f)\n",
        "\n",
        "# Load label encoder\n",
        "with open('models/label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Load best model\n",
        "with open('models/best_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "print(\"Model and vectorizers loaded successfully!\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Label classes: {label_encoder.classes_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prediction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(review_text):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a given review text\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    review_text : str\n",
        "        The movie review text to analyze\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing prediction, probability, and confidence\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(review_text)\n",
        "    \n",
        "    # Extract textual features\n",
        "    char_count = len(cleaned_text)\n",
        "    word_count = len(cleaned_text.split())\n",
        "    avg_word_length = char_count / (word_count + 1) if word_count > 0 else 0\n",
        "    exclamation_count = cleaned_text.count('!')\n",
        "    question_count = cleaned_text.count('?')\n",
        "    \n",
        "    # Transform text using TF-IDF\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "    \n",
        "    # Scale textual features\n",
        "    textual_features = np.array([[char_count, word_count, avg_word_length, \n",
        "                                  exclamation_count, question_count]])\n",
        "    textual_features_scaled = feature_scaler.transform(textual_features)\n",
        "    \n",
        "    # Combine features\n",
        "    from scipy.sparse import hstack\n",
        "    features = hstack([text_tfidf, textual_features_scaled])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(features)[0]\n",
        "    sentiment = label_encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    # Get prediction probability if available\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        probabilities = model.predict_proba(features)[0]\n",
        "        prob_dict = {label_encoder.classes_[i]: probabilities[i] \n",
        "                    for i in range(len(label_encoder.classes_))}\n",
        "        confidence = max(probabilities)\n",
        "    else:\n",
        "        prob_dict = None\n",
        "        confidence = None\n",
        "    \n",
        "    return {\n",
        "        'sentiment': sentiment,\n",
        "        'prediction': prediction,\n",
        "        'probabilities': prob_dict,\n",
        "        'confidence': confidence,\n",
        "        'original_text': review_text,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }\n",
        "\n",
        "print(\"Prediction function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Predict Sentiment for Sample Reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample reviews for testing\n",
        "sample_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting was superb and the storyline kept me engaged throughout. Highly recommended!\",\n",
        "    \"I was really disappointed with this film. The plot was confusing and the characters were poorly developed. Not worth watching.\",\n",
        "    \"The movie was okay. Nothing special, but not terrible either. It's a decent watch if you have nothing else to do.\",\n",
        "    \"Amazing cinematography and brilliant performances by all actors. This is one of the best movies I've seen this year!\",\n",
        "    \"Terrible movie. Boring plot, bad acting, and a complete waste of time. I would not recommend this to anyone.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICTING SENTIMENT FOR SAMPLE REVIEWS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    result = predict_sentiment(review)\n",
        "    \n",
        "    print(f\"\\nReview {i}:\")\n",
        "    print(f\"Text: {review[:100]}...\")\n",
        "    print(f\"Predicted Sentiment: {result['sentiment'].upper()}\")\n",
        "    if result['probabilities']:\n",
        "        print(f\"Probabilities: {result['probabilities']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Interactive Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter your own review here\n",
        "new_review = input(\"Enter a movie review to analyze: \")\n",
        "\n",
        "if new_review.strip():\n",
        "    result = predict_sentiment(new_review)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PREDICTION RESULT\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nReview: {new_review}\")\n",
        "    print(f\"\\nPredicted Sentiment: {result['sentiment'].upper()}\")\n",
        "    \n",
        "    if result['probabilities']:\n",
        "        print(f\"\\nConfidence: {result['confidence']:.2%}\")\n",
        "        print(f\"\\nDetailed Probabilities:\")\n",
        "        for sentiment, prob in result['probabilities'].items():\n",
        "            print(f\"  {sentiment.capitalize()}: {prob:.2%}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No review entered.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Batch Prediction from File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Predict sentiment for multiple reviews from a CSV file\n",
        "# Uncomment and modify the code below to use your own file\n",
        "\n",
        "\"\"\"\n",
        "# Load reviews from CSV file\n",
        "reviews_df = pd.read_csv('path/to/your/reviews.csv')\n",
        "\n",
        "# Make predictions\n",
        "predictions = []\n",
        "for review in reviews_df['review']:\n",
        "    result = predict_sentiment(review)\n",
        "    predictions.append(result['sentiment'])\n",
        "\n",
        "# Add predictions to dataframe\n",
        "reviews_df['predicted_sentiment'] = predictions\n",
        "\n",
        "# Save results\n",
        "reviews_df.to_csv('predictions_results.csv', index=False)\n",
        "print(f\"Predictions saved for {len(reviews_df)} reviews!\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Features:\n",
        "1. ✅ Load trained model and preprocessing components\n",
        "2. ✅ Preprocess new review text\n",
        "3. ✅ Make sentiment predictions\n",
        "4. ✅ Get prediction probabilities and confidence scores\n",
        "5. ✅ Support for single and batch predictions\n",
        "\n",
        "### Usage Tips:\n",
        "- The model works best with reviews similar to the training data\n",
        "- Longer reviews generally provide better predictions\n",
        "- The confidence score indicates how certain the model is about its prediction\n",
        "- Lower confidence scores may indicate ambiguous reviews\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
