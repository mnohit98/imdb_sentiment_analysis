{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Movie Review Sentiment Analysis - Part 1: Data Exploration & Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook covers:\n",
    "1. Data Loading and Initial Exploration\n",
    "2. Data Quality Assessment\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Text Preprocessing\n",
    "5. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NLTK data downloaded/verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/imdb_data.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"\n",
    "Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"\n",
    "Column names: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing Values Found:\")\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(\"\n",
    "Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Target Variable Analysis (Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "sentiment_percentages = df['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(sentiment_counts)\n",
    "print(\"\n",
    "Sentiment Percentages:\")\n",
    "print(sentiment_percentages)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar chart\n",
    "sentiment_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "sentiment_percentages.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Sentiment Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Review Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review length statistics\n",
    "df['review_length'] = df['review'].str.len()\n",
    "df['word_count'] = df['review'].str.split().str.len()\n",
    "\n",
    "print(\"Review Length Statistics:\")\n",
    "print(df[['review_length', 'word_count']].describe())\n",
    "\n",
    "# Visualize review length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Character count distribution\n",
    "df.boxplot(column='review_length', by='sentiment', ax=axes[0])\n",
    "axes[0].set_title('Review Length (Characters) by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=10)\n",
    "axes[0].set_ylabel('Character Count', fontsize=10)\n",
    "\n",
    "# Word count distribution\n",
    "df.boxplot(column='word_count', by='sentiment', ax=axes[1])\n",
    "axes[1].set_title('Word Count by Sentiment', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Sentiment', fontsize=10)\n",
    "axes[1].set_ylabel('Word Count', fontsize=10)\n",
    "\n",
    "plt.suptitle('Review Length Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for positive and negative reviews\n",
    "def create_wordcloud(text, title, ax):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Combine all positive reviews\n",
    "positive_text = ' '.join(df[df['sentiment'] == 'positive']['review'].astype(str))\n",
    "\n",
    "# Combine all negative reviews\n",
    "negative_text = ' '.join(df[df['sentiment'] == 'negative']['review'].astype(str))\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "create_wordcloud(positive_text, 'Positive Reviews Word Cloud', axes[0])\n",
    "create_wordcloud(negative_text, 'Negative Reviews Word Cloud', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('models', exist_ok=True)\n",
    "plt.savefig('models/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and digits (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stop words from tokens\"\"\"\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def lemmatize_text(tokens):\n",
    "    \"\"\"Lemmatize tokens (convert to base form)\"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def preprocess_text(text, use_lemmatization=True, remove_stop=True):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Lemmatize\n",
    "    if use_lemmatization:\n",
    "        tokens = lemmatize_text(tokens)\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Text preprocessing functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Apply Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to reviews\n",
    "print(\"Starting text preprocessing...\")\n",
    "print(\"This may take a few minutes for large datasets...\")\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Apply preprocessing (using lemmatization)\n",
    "df_processed['cleaned_review'] = df_processed['review'].apply(\n",
    "    lambda x: preprocess_text(x, use_lemmatization=True, remove_stop=True)\n",
    ")\n",
    "\n",
    "print(\"\n",
    "Text preprocessing completed!\")\n",
    "print(f\"\n",
    "Sample original review:\")\n",
    "print(df_processed['review'].iloc[0][:200])\n",
    "print(f\"\n",
    "\n",
    "Sample cleaned review:\")\n",
    "print(df_processed['cleaned_review'].iloc[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save full dataframe with cleaned reviews\n",
    "df_processed[['cleaned_review', 'sentiment']].to_csv('data/processed_reviews.csv', index=False)\n",
    "\n",
    "# Also save original for reference\n",
    "df_processed.to_csv('data/full_processed_data.csv', index=False)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- data/processed_reviews.csv\")\n",
    "print(\"- data/full_processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. Dataset loaded and explored\n",
    "2. Sentiment distribution analyzed\n",
    "3. Review length statistics calculated\n",
    "4. Text preprocessing completed (cleaning, tokenization, lemmatization)\n",
    "5. Preprocessed data saved for feature engineering\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to Feature Engineering notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
